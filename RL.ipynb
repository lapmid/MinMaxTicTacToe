{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"7gmgqSTRaoL_","executionInfo":{"status":"ok","timestamp":1696135884893,"user_tz":-330,"elapsed":17,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"}}},"outputs":[],"source":["import numpy as np\n","import random\n","import pickle\n","import json"]},{"cell_type":"markdown","metadata":{"id":"cGCU4sB0hlfI"},"source":["1. get possible moves\n","2. get max q value for each move\n","3. check winner or draw\n","4. play an random move then check for winner or draw\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1696135887609,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"},"user_tz":-330},"id":"e0BJcBzYarqv"},"outputs":[],"source":["class Game():\n","  def __init__(self,CP):\n","    self.board = '         '\n","    self.CP = CP\n","\n","  def get_opponent(CP):\n","    return 'X' if CP == 'O' else 'O';\n","\n","  def get_state(self):\n","    return ''.join(self.board);\n","\n","  def get_moves(self):\n","    return [index for index, char in enumerate(self.board) if char == ' ']\n","\n","  def make_move(self,pos,computer):\n","    self.board = self.board[:pos]+self.CP+self.board[pos+1:]\n","    winner = Game.check_winner(self.board)\n","    done = winner is not None or ' ' not in self.board\n","    reward = 0.0\n","    if winner is not None and winner == computer:\n","      reward = 1.0\n","    elif winner is not None and winner != computer:\n","      reward = -1.0\n","\n","    self.CP = 'O' if self.CP == 'X' else 'X'\n","    return self.get_state(),winner,reward,done\n","\n","  def is_valid(self,pos,player):\n","    return 0<=pos<=8 and self.board[pos] == ' '\n","\n","  def check_winner(gameState):\n","    lines = [[0,1,2],[3,4,5],[6,7,8],[0,3,6],[1,4,7],[2,5,8],[0,4,8],[2,4,6]]\n","    for line in lines:\n","      st = gameState[line[0]] + gameState[line[1]] + gameState[line[2]]\n","      if  st == 'XXX':\n","        return 'X'\n","      elif st == 'OOO':\n","        return 'O'\n","    return None\n","\n","  def print(self):\n","    for i in range(9):\n","      if i%3==0:\n","        print(\"\\n\",end=' ')\n","      if self.board[i] == ' ':\n","        print('- ',end=' ')\n","      else:\n","        print(self.board[i]+' ',end=' ')\n","    print('')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"51uPKUSlfjYI","executionInfo":{"status":"ok","timestamp":1696135892392,"user_tz":-330,"elapsed":462,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"}}},"outputs":[],"source":["\n","class QLearningAgent():\n","  def __init__(self, learning_rate=0.1, discount_factor=0.95, exploration_rate=0.5):\n","    self.learning_rate = learning_rate  # Alpha: Learning rate\n","    self.discount_factor = discount_factor  # Gamma: Discount factor\n","    self.exploration_rate = exploration_rate  # Epsilon: Exploration rate\n","\n","    # Initialize the Q-table as a dictionary with default Q-values of 0.0\n","    self.q_table = {}\n","    self.q_table_json = {}\n","\n","  def choose_action(self, state, actions, CP):\n","    # Explore (random action) or exploit (best known action) based on epsilon\n","    if random.uniform(0, 1) < self.exploration_rate:\n","      return random.choice(actions)\n","    else:\n","      return self.get_best_action(state,actions,CP)\n","\n","  def get_danger(self,state,actions,CP):\n","    OP = Game.get_opponent(CP)\n","    res = []\n","    for move in actions:\n","      winner = Game.check_winner(state[:move]+OP+state[move+1:])\n","      if winner == OP:\n","        res.append(move)\n","    return res\n","\n","  def get_best_action(self, state, actions, CP):\n","    # Find the action with the highest Q-value for the given state\n","    best_action = None\n","    best_q_value = float('-inf')  # Initialize with negative infinity\n","\n","    # check if we can win\n","    for action in actions:\n","        winner = Game.check_winner(state[:action]+CP+state[action+1:])\n","        if winner == CP:\n","          return action\n","\n","    # check danger positions\n","    OP = Game.get_opponent(CP)\n","    dangers = self.get_danger(state,actions,CP)\n","    if len(dangers)>0:\n","      return random.choice(dangers)\n","    else:\n","      # get best move\n","      for action in actions:\n","        q_value = self.get_q_value(state, action, CP)\n","        if q_value > best_q_value:\n","          best_action = action\n","          best_q_value = q_value\n","\n","    if best_action is None:\n","      return random.choice(actions)\n","\n","    # get all possible best moves and select randomly\n","    best_actions = []\n","    for action in actions:\n","      q_value = self.get_q_value(state, action, CP)\n","      if q_value == best_q_value:\n","        best_actions.append(action)\n","\n","    return random.choice(best_actions)\n","\n","  def get_q_value(self, state, action, CP):\n","    # Retrieve the Q-value for a specific state-action pair\n","    return self.q_table.get((state, str(action)+CP), 0.0)\n","\n","  def update_q_value(self, state, action, new_q_value, CP):\n","    # Update the Q-value for a specific state-action pair\n","    self.q_table[(state, str(action)+CP)] = new_q_value\n","    self.q_table_json[state+\"|\"+str(action)+CP] = new_q_value\n","\n","  def learn(self, CP, OP, computer, state, action, reward, next_state, actions, winner):\n","    # Q-learning update rule to adjust Q-values\n","    current_q_value = self.get_q_value(state, action, CP)\n","    max_q_value = 0.0\n","    # if winner is not None and winner == computer:\n","    #   max_q_value = 2 * reward\n","    # elif winner is not None and winner != computer:\n","    #   max_q_value = -5 * reward\n","\n","    if len(actions)>0:\n","      max_q_value = max([self.get_q_value(next_state, a, OP) for a in actions])\n","\n","    new_q_value = (1 - self.learning_rate) * current_q_value + self.learning_rate * ( reward + self.discount_factor * max_q_value )\n","    # new_q_value = (1 - self.learning_rate) * current_q_value + self.learning_rate * (reward + self.discount_factor * max_q_value)\n","    self.update_q_value(state, action, new_q_value, CP)\n","\n","  def save_agent(self, filename):\n","    # Saving the Trained Model\n","    with open(filename+'.json', 'w') as json_file:\n","      json.dump(self.q_table_json, json_file)\n","    with open(filename, 'wb') as f:\n","      pickle.dump(self.q_table, f)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Jvc6PWZhrbIo","executionInfo":{"status":"ok","timestamp":1696136269170,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"}}},"outputs":[],"source":["class Train():\n","  def __init__(self):\n","    self.agent = QLearningAgent()\n","    self.agentTrained = QLearningAgent()\n","    self.init()\n","\n","  def init(self, filename=\"RLModelTrained\"):\n","    # Loading the Trained Model\n","    with open(filename, 'rb') as f:\n","      self.agentTrained.q_table = pickle.load(f)\n","\n","  def train(self,ite):\n","    for i in range(ite):\n","      self.playGame(i)\n","      if i%50000 == 0:\n","        print('learnt ' + str(i))\n","      if i>100000:\n","        self.agent.exploration_rate = 0.6\n","      if i>200000:\n","        self.agent.exploration_rate = 0.4\n","      if i>300000:\n","        self.agent.exploration_rate = 0.2\n","      if i>400000:\n","        self.agent.exploration_rate = 0.1\n","    self.agent.save_agent(\"RLModel\")\n","\n","  def trainMinMax(self):\n","    game= Game('X')\n","\n","    for i in range(8):\n","      score = self.minmax(game,1,True)\n","\n","  def playGame(self,i):\n","    game = Game('X' if i%2==0 else 'O')\n","    done = False\n","    computer = 'X'\n","    while not done:\n","      CP = game.CP\n","      OP = Game.get_opponent(CP)\n","      pState = game.get_state()\n","      action = None\n","\n","      if CP != computer:\n","        action = self.agentTrained.get_best_action(pState, game.get_moves(), CP)\n","      else:\n","        action = self.agent.choose_action(pState, game.get_moves(), CP)\n","\n","      nState,winner,reward,done = game.make_move(action,computer)\n","      possibleMoves = []\n","      if not done:\n","        possibleMoves = game.get_moves()\n","      self.agent.learn(CP, OP, computer, pState,action,reward,nState,possibleMoves,winner)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-_0Bn209yVy","executionInfo":{"status":"ok","timestamp":1696136497929,"user_tz":-330,"elapsed":222367,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"}},"outputId":"d4d9ac15-761d-4883-9ce2-cca34fb937a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["learnt 0\n","learnt 50000\n","learnt 100000\n","learnt 150000\n","learnt 200000\n","learnt 250000\n","learnt 300000\n","learnt 350000\n","learnt 400000\n","learnt 450000\n"]}],"source":["train = Train()\n","train.train(500000)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1696136208961,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"},"user_tz":-330},"id":"bLAaA8JMBoD2"},"outputs":[],"source":["class GamePlayer():\n","  def __init__(self):\n","    self.agent = None\n","    self.CNT = {}\n","    self.D = 0\n","    self.init()\n","\n","  def init(self, filename=\"RLModel\"):\n","    # Loading the Trained Model\n","    self.agent = QLearningAgent()\n","    with open(filename, 'rb') as f:\n","      self.agent.q_table = pickle.load(f)\n","\n","  def play(self,i,computer,log=True):\n","    game = Game('X' if i%2==0 else 'O')\n","    done = False\n","    while not done:\n","      moves = game.get_moves()\n","      pState = game.get_state()\n","      move = None\n","      CP = game.CP\n","      move = None\n","      if CP != computer:\n","        move = random.choice(moves)\n","      else:\n","        move = self.agent.get_best_action(pState, moves, CP)\n","      nState,winner,reward,done = game.make_move(move,computer)\n","      # print(nState, winner)\n","      if log:\n","        game.print()\n","      if winner != None:\n","        if log:\n","          print(\"Player \"+winner+\" Won\")\n","        self.CNT[winner] = self.CNT.get(winner,0)+1\n","        break\n","      elif done:\n","        if log:\n","          print(\"Game drawn\")\n","        self.D = self.D + 1\n","        break\n","      if log:\n","        print()\n","\n","  def printQ(self,state,CP,moves):\n","    for i in moves:\n","      print(self.agent.get_q_value(state,i,CP))\n","\n","  def printStat(self):\n","    print(\"Player X won \" + str(self.CNT.get('X')))\n","    print(\"Player O won \" + str(self.CNT.get('O')))\n","    print(\"Draw \" + str(self.D))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":564,"status":"ok","timestamp":1695750958241,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"},"user_tz":-330},"id":"9GKo8DR5oT4J","outputId":"82c38152-5b56-4d9f-8c83-181450b56ac5"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0\n","0.0\n","0.0\n","0.0\n","0.0\n"]}],"source":["player = GamePlayer()\n","player.printQ('XXO O    ','X',[3,5,6,7,8])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uC-tVw3nFEw"},"outputs":[],"source":["player = GamePlayer()\n","player.play(1,'X')"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1696136579690,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"},"user_tz":-330},"id":"c87qer96ES29","outputId":"2f88ffcc-085d-40d0-c23c-2fb43b976c1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Player X won 827\n","Player O won 38\n","Draw 135\n"]}],"source":["cnt = 0.0\n","player = GamePlayer()\n","for i in range(1000):\n","  player.play(i,'X',log = False)\n","player.printStat()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1695994265475,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"},"user_tz":-330},"id":"Qfs00TqjFLX-","outputId":"d58d257e-b59a-4e63-bfd8-d8a709d61cdf"},"outputs":[{"name":"stdout","output_type":"stream","text":["O\n"]}],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5PxE88XL2TFeBY7EA/5I4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}