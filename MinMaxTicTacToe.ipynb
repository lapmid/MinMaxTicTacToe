{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1ZGup2nsdDsu7o2qjyED_NvCwXq508mST","authorship_tag":"ABX9TyMIWnYfcuhZknvtOf00/Mqy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"k5AbPPJVzvj4","executionInfo":{"status":"ok","timestamp":1696139924453,"user_tz":-330,"elapsed":22,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"}}},"outputs":[],"source":["import numpy as np\n","import random\n","import pickle\n","import json"]},{"cell_type":"code","source":["class MinMaxAgent():\n","  def __init__(self):\n","    self.q_table = {}\n","    self.board = '         '\n","\n","  def get_best_move(self, state):\n","    # Retrieve the Q-value for a specific state-action pair\n","    return self.q_table.get(state, -1)\n","\n","  def update_best_move(self, state, action):\n","    self.q_table[state] = action\n","\n","  def get_moves(self):\n","    return [m for m in range(9) if self.board[m]==' ']\n","\n","  def check_winner(gameState):\n","    lines = [[0,1,2],[3,4,5],[6,7,8],[0,3,6],[1,4,7],[2,5,8],[0,4,8],[2,4,6]]\n","    winner = None\n","    for line in lines:\n","      st = gameState[line[0]] + gameState[line[1]] + gameState[line[2]]\n","      if  st == 'XXX':\n","        winner = 'X'\n","      elif st == 'OOO':\n","        winner = 'O'\n","    done = False\n","    if winner is not None or ' ' not in gameState:\n","      done = True\n","\n","    return winner,done\n","\n","  def get_op(self,CP):\n","    return 'X' if CP == 'O' else 'O'\n","\n","  def trainMinMax(self,CP,computer):\n","    self.minmax(0,CP,computer)\n","\n","  def minmax(self,depth,CP,computer):\n","    # if self.board == 'X   O X O':\n","    #   print('yes')\n","    winner,done = MinMaxAgent.check_winner(self.board)\n","    if done:\n","      if winner is not None and winner == computer:\n","        return 10 - depth,None\n","      elif winner is not None and winner != computer:\n","        return -10 + depth,None\n","      else:\n","        return 0,None\n","\n","    moves = self.get_moves()\n","    best_move = None\n","    best_score = -200 if CP == computer else 200\n","    for m in moves:\n","      self.board = self.board[:m]+CP+self.board[m+1:]\n","      score,moves = self.minmax(depth+1,self.get_op(CP),computer)\n","      self.board = self.board[:m]+' '+self.board[m+1:]\n","      if CP == computer:\n","        if score > best_score:\n","          best_move = m\n","          best_score = score\n","      else:\n","        if score < best_score:\n","          best_move = m\n","          best_score = score\n","    # if self.board == 'X  OO X O':\n","    #   print(best_score,best_move)\n","    self.update_best_move(self.board,best_move)\n","    return best_score,best_move\n","\n","  def save_agent(self, filename='MinMaxModel'):\n","    # Saving the Trained Model\n","    with open(filename+'.json', 'w') as json_file:\n","      json.dump(self.q_table, json_file)\n","    with open(filename, 'wb') as f:\n","      pickle.dump(self.q_table, f)"],"metadata":{"id":"YnIM_Jc9sMPC","executionInfo":{"status":"ok","timestamp":1696143836837,"user_tz":-330,"elapsed":460,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["agent = MinMaxAgent()\n","# agent.board = 'XXO O XO '\n","# score,move = agent.minmax(2,'X','X')\n","# print(score,move)\n","# agent.trainMinMax('X','X')\n","agent.trainMinMax('X','X')\n","agent.save_agent('MinMaxModelPlayer2')\n","# print(agent.q_table)"],"metadata":{"id":"AHXGxTLi0DNP","executionInfo":{"status":"ok","timestamp":1696144173648,"user_tz":-330,"elapsed":2119,"user":{"displayName":"Vivek Pandey","userId":"04290673835230010340"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"vpAkO0Lw1Va8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLAaA8JMBoD2"},"outputs":[],"source":["class GamePlayer():\n","  def __init__(self):\n","    self.agent = None\n","    self.CNT = {}\n","    self.D = 0\n","    self.init()\n","\n","  def init(self, filename=\"RLModel\"):\n","    # Loading the Trained Model\n","    self.agent = QLearningAgent()\n","    with open(filename, 'rb') as f:\n","      self.agent.q_table = pickle.load(f)\n","\n","  def play(self,i,computer,log=True):\n","    game = Game('X' if i%2==0 else 'O')\n","    done = False\n","    while not done:\n","      moves = game.get_moves()\n","      pState = game.get_state()\n","      move = None\n","      CP = game.CP\n","      move = None\n","      if CP != computer:\n","        move = random.choice(moves)\n","      else:\n","        move = self.agent.get_best_action(pState, moves, CP)\n","      nState,winner,reward,done = game.make_move(move,computer)\n","      # print(nState, winner)\n","      if log:\n","        game.print()\n","      if winner != None:\n","        if log:\n","          print(\"Player \"+winner+\" Won\")\n","        self.CNT[winner] = self.CNT.get(winner,0)+1\n","        break\n","      elif done:\n","        if log:\n","          print(\"Game drawn\")\n","        self.D = self.D + 1\n","        break\n","      if log:\n","        print()\n","\n","  def printQ(self,state,CP,moves):\n","    for i in moves:\n","      print(self.agent.get_q_value(state,i,CP))\n","\n","  def printStat(self):\n","    print(\"Player X won \" + str(self.CNT.get('X')))\n","    print(\"Player O won \" + str(self.CNT.get('O')))\n","    print(\"Draw \" + str(self.D))"]}]}